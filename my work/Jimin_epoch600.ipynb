{"cells":[{"cell_type":"markdown","metadata":{"id":"GWSi3kVypwIh"},"source":["# 2 model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMVOdp23pwIh"},"outputs":[],"source":["class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","\n","def visual(img, ground_truth, prediction, gpu = 0, flag_torch = 0):\n","  if gpu:\n","    img = img.cpu().detach()\n","    ground_truth = ground_truth.cpu().detach()\n","    prediction = prediction.cpu().detach()\n","\n","  if flag_torch:\n","    img = img.numpy()\n","    ground_truth = ground_truth.numpy()\n","    prediction = prediction.numpy()\n","\n","  if flag_torch == 2:\n","    img = np.transpose(img[:, :, :, :, :], [0, 4, 2, 3, 1]).squeeze()\n","\n","  #correct image structure\n","  #use last 5 images so we can easily compare between batch sizes\n","  img = np.transpose(img[-5:, :, :, :], [0, 2, 3, 1])\n","  ground_truth = ground_truth[-5:, :, :]\n","  prediction = prediction[-5:, :, :]\n","\n","  for i in range(5):\n","      ax = plt.subplot(3, 5, i + 1)\n","      ax.imshow(img[i])\n","      ax.axis(\"off\")\n","      ax = plt.subplot(3, 5, i + 1 + 5)\n","      ax.imshow(ground_truth[i])\n","      ax.axis(\"off\")\n","      ax = plt.subplot(3, 5, i + 1 + 10)\n","      ax.imshow(prediction[i])\n","      ax.axis(\"off\")\n","  plt.show()\n","\n","def train(args):\n","    Net = globals()[args.model]\n","    #model = Net(args.kernel, args.num_filters)\n","    model = Net(output_channels = 1)\n","    train_loader = torch.utils.data.DataLoader(args.data, batch_size=args.batch_size, shuffle=True)\n","\n","    # Save directory\n","    save_dir = \"outputs/\" + args.experiment_name\n","\n","    # LOSS FUNCTION\n","\n","    criterion1 = nn.BCELoss()\n","    criterion2 = nn.MSELoss()\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.learn_rate)\n","\n","\n","    # Create the outputs folder if not created already\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","\n","    print(\"Beginning training ...\")\n","    if args.gpu:\n","        model.cuda()\n","    start_time = time.time()\n","\n","    train_losses = []\n","    valid_losses = []\n","    valid_accs = []\n","    epochs = []\n","\n","    for epoch in range(args.epochs):\n","        # Train the Model\n","        model.train()  # Change model to 'train' mode\n","        losses = []\n","        for imgs, density, counts in iter(train_loader):\n","            # Forward + Backward + Optimize\n","            optimizer.zero_grad()\n","            outputs = model(imgs.cuda())\n","            outputs = outputs.squeeze(1)\n","\n","            loss = criterion2(outputs, density.cuda())\n","\n","\n","            loss.backward()\n","            optimizer.step()\n","            losses.append(loss.data.item())\n","        train_losses.append(sum(losses))\n","        epochs.append(epoch)\n","        print(epoch, train_losses[-1])\n","\n","        if args.visualize and (epoch+1) % args.visualize_epochs == 0:\n","            visual(imgs, density, outputs, args.gpu, 1)\n","    if args.plot:\n","        # plotting\n","        plt.title(\"Training Curve\")\n","        plt.plot(epochs[10:], train_losses[10:], label=\"Train\")\n","        plt.xlabel(\"Epochs\")\n","        plt.ylabel(\"Loss\")\n","        plt.show()\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n","    torch.save(model.state_dict(), os.path.join(save_dir, \"final\"))\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"G2R__LV0pwIj"},"source":["## define model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ayk-i0upwIj"},"outputs":[],"source":["class ASPP(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(ASPP, self).__init__()\n","        self.conv_1x1_1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.conv_3x3_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=6, dilation=6)\n","        self.conv_3x3_2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=12, dilation=12)\n","        self.conv_3x3_3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=18, dilation=18)\n","        self.conv_1x1_2 = nn.Conv2d(out_channels * 4, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        x1 = self.conv_1x1_1(x)\n","        x2 = self.conv_3x3_1(x)\n","        x3 = self.conv_3x3_2(x)\n","        x4 = self.conv_3x3_3(x)\n","        x = torch.cat((x1, x2, x3, x4), dim=1)\n","        x = self.conv_1x1_2(x)\n","        return x\n","\n","class VGG16Modified2(nn.Module):\n","    def __init__(self, output_channels=1):\n","        super(VGG16Modified2, self).__init__()\n","        self.features = models.vgg16(pretrained=True).features\n","        for param in self.features.parameters():\n","            param.requires_grad = False\n","\n","        self.aspp = ASPP(512, 256)\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(16, output_channels, kernel_size=2, stride=2),\n","        )\n","\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.aspp(x)\n","        x = self.decoder(x)\n","\n","\n","        return x.squeeze()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fI7_UyCVpwIj","executionInfo":{"status":"ok","timestamp":1711287936793,"user_tz":240,"elapsed":9170,"user":{"displayName":"jimin kim","userId":"01228242279127684566"}},"outputId":"b684d268-5112-4193-8135-29f0d8ad3e36"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:05<00:00, 106MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Output size: torch.Size([224, 224])\n"]}],"source":["# test output size\n","model = VGG16Modified2(output_channels=1)\n","\n","# dummy input tensor\n","dummy_input = torch.randn(1, 3, 224, 224)\n","\n","with torch.no_grad():\n","    dummy_output = model(dummy_input)\n","\n","print(\"Output size:\", dummy_output.size())"]},{"cell_type":"markdown","source":["### epoch = 600"],"metadata":{"id":"Qxv25eGatges"}},{"cell_type":"code","source":["model = VGG16Modified2()\n","state = torch.load(\"/content/drive/MyDrive/UT/IDL/project/my_VGG16_mseloss_epoch600\")\n","model.load_state_dict(state)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCjSixSmtgJH","executionInfo":{"status":"ok","timestamp":1711288004183,"user_tz":240,"elapsed":6894,"user":{"displayName":"jimin kim","userId":"01228242279127684566"}},"outputId":"4e848ef6-2636-4a33-8d6e-bd52baec74e9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["model.cuda()\n","model.eval()\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=5, shuffle=False)\n","i = 0\n","\n","total_mae = 0.0  # total MAE\n","total_count = 0  # total precossed sample\n","\n","for imgs, density, counts in iter(test_loader):\n","    outputs = model(imgs.cuda())\n","    pred = outputs\n","\n","    pred_counts = torch.sum(outputs, [1,2])\n","\n","    mae = torch.abs(pred_counts - counts.cuda()).sum().item()\n","    total_mae += mae\n","    total_count += counts.size(0)\n","\n","    visual(imgs, density, pred, True, 1)\n","    print(counts)\n","    print(torch.round(torch.sum(density, [1,2])))\n","    print(torch.round(torch.sum(pred, [1,2])))\n","\n","# average mae of total test set\n","mean_mae = total_mae / total_count\n","print(f\"Mean Absolute Error (MAE): {mean_mae}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"11XLrIekwOhuOkR2QJKmLkyNOAuixUGDl"},"id":"xOGRZXSpts0I","executionInfo":{"status":"ok","timestamp":1711288108993,"user_tz":240,"elapsed":39834,"user":{"displayName":"jimin kim","userId":"01228242279127684566"}},"outputId":"13a35a5e-88f3-45e0-8f96-09c1a44ca9ff"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# Mean Absolute Error (MAE): 22.051281606739966 -> epoch = 600"],"metadata":{"id":"OBWILuaCuFqr"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}